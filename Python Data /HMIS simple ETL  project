import argparse
import hashlib
import logging
import re
import time
from fnmatch import fnmatch
from glob import glob
from pathlib import Path
from tempfile import NamedTemporaryFile
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd


# -----------------------------
# Configuration
# -----------------------------
DEFAULT_BASE = r"C:\Users\ElmarthyJanettyGalla\OneDrive - Weingart Center Association\QA Data Hub"


def make_cfg(base: str) -> Dict[str, Any]:
    return {
        "paths": {
            "staging2": fr"{base}\Report Outputs\Second Staging Area",
            "output": fr"{base}\Report Outputs\Output",
            "state_dir": fr"{base}\_pipeline_state",
        },
        "reader_defaults": {
            "encoding": "utf-8",
            "csv": {"header": 0, "na_values": ["", "NA", "N/A", "null", "None"]},
            "excel": {"sheet": "first", "header_row": 0},
        },
        "datasets": [
            {
                "name": "program_client_data",
                "staging1": fr"{base}\Staging Area\Biweekly Report\Program Client Data",
                "include_patterns": ["*.csv", "*.xlsx", "*.xls"],
                "exclude_patterns": ["~$*", "*.tmp"],
                "combined_filename": "program_client_data.parquet",
                "renames": {
                    "Clients Unique Identifier": "ClientID",
                    "Enrollments Active in Project": "ActiveInProject",
                    "Clients Client Full Name": "ClientFullName",
                    "Clients Active ROI?": "ActiveROI",
                    "Enrollments Days in Project": "DaysInProject",
                    "Enrollments Project Start Date": "ProjectStartDate",
                    "Client Custom Point of Contact Name": "POCName",
                    "Client Custom Point of Contact Phone": "POCPhone",
                    "Client Custom Point of Contact Email": "POCEmail",
                    "Client Custom Point of Contact Date": "POCDate",
                    "Clients DoB Data Quality": "DOBDataQuality",
                    "Clients SSN Data Quality": "SSNDataQuality",
                    "Clients SSN - Last 4": "SSN_Last4",
                    "Enrollments Deleted (Yes / No)": "EnrollmentDeleted",
                    "Client Assessment Custom TB Clearance Date": "TB_ClearanceDate",
                    "Programs Full Name": "ProgramName",
                    "Client Assessments Last Assessment ID": "LastAssessmentID",
                    "Client Assessments Last Assessment Date": "LastAssessmentDate",
                    "List of Client File Name": "ClientFileList",
                    "List of Assigned Staff": "AssignedStaff",
                },
                "date_rules": [
                    {"column": "ProjectStartDate", "formats": ["%m/%d/%Y", "%Y-%m-%d"]},
                    {"column": "POCDate", "formats": ["%m/%d/%Y", "%Y-%m-%d"]},
                    {"column": "LastAssessmentDate", "formats": ["%m/%d/%Y", "%Y-%m-%d"]},
                    {"column": "TB_ClearanceDate", "formats": ["%m/%d/%Y", "%Y-%m-%d"]},
                ],
                "dtypes": {
                    "ClientID": "string",
                    "ClientFullName": "string",
                    "ActiveROI": "string",
                    "ActiveInProject": "string",
                    "DaysInProject": "Int64",
                    "POCName": "string",
                    "POCPhone": "string",
                    "POCEmail": "string",
                    "DOBDataQuality": "string",
                    "SSNDataQuality": "string",
                    "SSN_Last4": "string",
                    "EnrollmentDeleted": "string",
                    "ProgramName": "string",
                    "LastAssessmentID": "string",
                    "ClientFileList": "string",
                    "AssignedStaff": "string",
                },
                "lowercase_columns": ["POCEmail"],
                "drop_queries": ["ClientID.isna()"],
                "drop_null_any": [],
                "dedup_keys": ["ClientID", "ProgramName"],
                "recency_cols": ["POCDate", "LastAssessmentDate", "ProjectStartDate"],
            },
            {
                "name": "ces_assessments",
                "staging1": fr"{base}\Staging Area\Biweekly Report\CES Assessments",
                "include_patterns": ["*.csv", "*.xlsx", "*.xls"],
                "exclude_patterns": ["~$*", "*.tmp"],
                "combined_filename": "ces_assessments.parquet",
                "renames": {
                    "Clients Unique Identifier": "ClientID",
                    "Programs Full Name": "ProgramName",
                    "Client Assessments Assessment ID": "CES_AssessmentID",
                    "Client Assessments Assessment Date": "CES_AssessmentDate",
                    "Client Assessments Assessment Score": "CES_AssessmentScore",
                    "Client Assessments Is Coordinated Entry": "CES_IsCoordinatedEntry",
                },
                "date_rules": [
                    {"column": "CES_AssessmentDate", "formats": ["%Y-%m-%d", "%m/%d/%Y"]},
                ],
                "dtypes": {
                    "ClientID": "string",
                    "ProgramName": "string",
                    "CES_AssessmentID": "string",
                    "CES_AssessmentScore": "Int64",
                    "CES_IsCoordinatedEntry": "string",
                },
                "lowercase_columns": [],
                "drop_queries": ["ClientID.isna()"],
                "drop_null_any": [],
                "dedup_keys": ["ClientID", "ProgramName", "CES_AssessmentDate", "CES_AssessmentID"],
                "recency_cols": ["CES_AssessmentDate"],
            },
            {
                "name": "case_notes",
                "staging1": fr"{base}\Staging Area\Biweekly Report\Case Notes",
                "include_patterns": ["*.csv", "*.xlsx", "*.xls"],
                "exclude_patterns": ["~$*", "*.tmp"],
                "combined_filename": "case_notes.parquet",
                "renames": {},
                "date_rules": [
                    {"column": "Month", "formats": ["%Y-%m", "%Y-%m-%d"]},
                ],
                "dtypes": {
                    "ClientID": "string",
                    "ProgramName": "string",
                    "CaseNoteCount": "Int64",
                },
                "lowercase_columns": [],
                "drop_queries": ["ClientID.isna()"],
                "drop_null_any": [],
                "dedup_keys": ["ClientID", "ProgramName", "Month"],
                "recency_cols": [],
            },
            {
                "name": "services",
                "staging1": fr"{base}\Staging Area\Biweekly Report\Services",
                "include_patterns": ["*.csv", "*.xlsx", "*.xls"],
                "exclude_patterns": ["~$*", "*.tmp"],
                "combined_filename": "services.parquet",
                "renames": {
                    "Clients Unique Identifier": "ClientID",
                    "Programs Full Name": "ProgramName",
                    "Services Start Date Month": "ServicesMonth",
                    "Services Count": "ServicesCount",
                },
                "date_rules": [
                    {"column": "ServicesMonth", "formats": ["%Y-%m", "%m/%d/%Y", "%Y-%m-%d"]},
                ],
                "dtypes": {
                    "ClientID": "string",
                    "ProgramName": "string",
                    "ServicesCount": "Int64",
                },
                "lowercase_columns": [],
                "drop_queries": ["ClientID.isna()"],
                "drop_null_any": [],
                "dedup_keys": ["ClientID", "ProgramName", "ServicesMonth"],
                "recency_cols": [],
            },
        ],
        "output": {
            "final_view_basename": "WCA_Biweekly_Final",
            "write_csv": True,
            "write_parquet": True,
            "write_excel": True,
            "index": False,
        },
        "logging": {"level": "INFO"},
    }


# -----------------------------
# Utilities
# -----------------------------
def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def list_ingest_files(folder: Path, include: List[str], exclude: List[str]) -> List[Path]:
    hits: List[Path] = []
    for pat in include:
        hits.extend([Path(p) for p in glob(str(folder / pat))])

    files: List[Path] = []
    for f in hits:
        if any(fnmatch(f.name, ex) for ex in exclude):
            continue
        if f.is_file():
            files.append(f)

    return sorted(files)


def md5_file(path: Path, chunk_size: int = 1024 * 1024) -> str:
    h = hashlib.md5()
    with open(path, "rb") as f:
        while True:
            b = f.read(chunk_size)
            if not b:
                break
            h.update(b)
    return h.hexdigest()


def load_state(state_path: Path) -> Dict[str, Any]:
    if not state_path.exists():
        return {}
    try:
        return pd.read_json(state_path, typ="series").to_dict()  # compact and dependency-free
    except Exception:
        return {}


def save_state(state_path: Path, state: Dict[str, Any]) -> None:
    ensure_dir(state_path.parent)
    s = pd.Series(state)
    with NamedTemporaryFile("w", delete=False, dir=state_path.parent, suffix=".tmp", encoding="utf-8") as tf:
        tmp = Path(tf.name)
    s.to_json(tmp)
    atomic_replace(tmp, state_path)


def parse_date_from_name(name: str) -> Optional[pd.Timestamp]:
    """
    Attempts to parse a date from a filename stem.
    Supports: MM-DD-YYYY, MM_DD_YYYY, YYYY-MM-DD, YYYY_MM_DD
    """
    patterns = [
        r"(\d{1,2})[-_](\d{1,2})[-_](\d{4})",  # MM-DD-YYYY
        r"(\d{4})[-_](\d{1,2})[-_](\d{1,2})",  # YYYY-MM-DD
    ]
    for pat in patterns:
        m = re.findall(pat, name)
        if not m:
            continue
        a, b, c = m[-1]
        try:
            # decide which format matched by length of first token
            if len(a) == 4:
                yyyy, mm, dd = int(a), int(b), int(c)
            else:
                mm, dd, yyyy = int(a), int(b), int(c)
            return pd.Timestamp(year=yyyy, month=mm, day=dd)
        except Exception:
            continue
    return None


def pick_latest_file(files: List[Path]) -> Optional[Path]:
    if not files:
        return None

    def key(fp: Path) -> pd.Timestamp:
        d = parse_date_from_name(fp.stem)
        if d is not None:
            return d
        try:
            return pd.Timestamp.fromtimestamp(fp.stat().st_mtime)
        except Exception:
            return pd.Timestamp(1970, 1, 1)

    return sorted(files, key=key)[-1]


def read_csv(fp: Path, defaults: Dict[str, Any]) -> pd.DataFrame:
    encoding = defaults.get("encoding", "utf-8")
    csv_cfg = defaults.get("csv", {})
    try:
        return pd.read_csv(fp, encoding=encoding, **csv_cfg)
    except UnicodeDecodeError:
        return pd.read_csv(fp, encoding="cp1252", **csv_cfg)


def read_excel(fp: Path, defaults: Dict[str, Any]) -> pd.DataFrame:
    xl = defaults.get("excel", {})
    sheet = xl.get("sheet", "first")
    header = xl.get("header_row", 0)
    if isinstance(sheet, str) and sheet.lower() == "first":
        sheet = 0
    # Note: .xls may require 'xlrd' depending on your environment
    return pd.read_excel(fp, sheet_name=sheet, header=header)


def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    return df


def ensure_unique_columns(df: pd.DataFrame) -> pd.DataFrame:
    dup_mask = df.columns.duplicated(keep="first")
    if dup_mask.any():
        df = df.loc[:, ~dup_mask].copy()
    return df


def apply_column_renames(df: pd.DataFrame, renames: Dict[str, str]) -> pd.DataFrame:
    if not renames:
        return df
    exist = {k: v for k, v in renames.items() if k in df.columns}
    return df.rename(columns=exist)


def parse_dates(df: pd.DataFrame, date_rules: List[Dict[str, Any]]) -> pd.DataFrame:
    df = df.copy()
    for r in date_rules:
        col = r.get("column")
        if not col or col not in df.columns:
            continue

        series = df[col]
        fmts = r.get("formats") or []
        if fmts:
            out = pd.to_datetime(pd.Series([pd.NA] * len(df), index=df.index), errors="coerce")
            for fmt in fmts:
                attempt = pd.to_datetime(series, errors="coerce", format=fmt)
                out = out.fillna(attempt)
            df[col] = out
        else:
            df[col] = pd.to_datetime(series, errors="coerce")
    return df


def enforce_dtypes(df: pd.DataFrame, dtypes: Dict[str, str]) -> pd.DataFrame:
    df = df.copy()
    for col, dt in dtypes.items():
        if col not in df.columns:
            continue
        try:
            if dt == "string":
                df[col] = df[col].astype("string")
            elif dt in ("Int64", "int64"):
                df[col] = pd.to_numeric(df[col], errors="coerce").astype("Int64")
            else:
                df[col] = df[col].astype(dt)
        except Exception:
            # keep original if coercion fails
            pass
    return df


def trim_strings(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    obj_cols = df.select_dtypes(include=["object", "string"]).columns
    for c in obj_cols:
        df[c] = df[c].astype("string").str.strip()
    return df


def to_lower(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    df = df.copy()
    for c in cols:
        if c in df.columns:
            df[c] = df[c].astype("string").str.lower()
    return df


def drop_bad_rows(df: pd.DataFrame, queries: List[str], null_any: List[str]) -> pd.DataFrame:
    """
    Supports simple query expressions. Defaults in config use ClientID.isna().
    """
    if df.empty:
        return df

    mask = pd.Series(True, index=df.index)
    for q in queries:
        try:
            # df.eval returns a boolean series for many expressions
            bad = df.eval(q, engine="python")
            if isinstance(bad, pd.Series) and bad.dtype == bool:
                mask &= ~bad
        except Exception:
            # fallback: ignore invalid expressions
            continue

    for col in null_any:
        if col in df.columns:
            mask &= df[col].notna()

    return df.loc[mask].copy()


def deduplicate(df: pd.DataFrame, keys: List[str], recency_cols: List[str]) -> pd.DataFrame:
    if df.empty:
        return df

    sort_cols = [c for c in recency_cols if c in df.columns]
    if sort_cols:
        df = df.sort_values(sort_cols, na_position="last")

    if keys:
        keep_cols = [k for k in keys if k in df.columns]
        if keep_cols:
            df = df.drop_duplicates(subset=keep_cols, keep="last")
    else:
        df = df.drop_duplicates(keep="last")

    return df.reset_index(drop=True)


# -----------------------------
# Case Notes: reshape to long
# -----------------------------
def preprocess_case_notes_to_long(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=["ClientID", "ProgramName", "Month", "CaseNoteCount"])

    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]

    if {"ClientID", "ProgramName", "Month", "CaseNoteCount"}.issubset(df.columns):
        return df[["ClientID", "ProgramName", "Month", "CaseNoteCount"]].copy()

    # Some exports appear to have an extra header row
    if "Clients Unique Identifier" not in df.columns and len(df) >= 3:
        new_cols = df.iloc[1].tolist()
        df = df.iloc[2:].copy()
        df.columns = [str(c).strip() for c in new_cols]

    df.columns = [str(c).strip() for c in df.columns]
    lower_map = {c.lower(): c for c in df.columns}

    def pick(possibles: List[str]) -> Optional[str]:
        for p in possibles:
            if p.lower() in lower_map:
                return lower_map[p.lower()]
        return None

    id_col = pick(["ClientID", "Clients Unique Identifier"])
    prg_col = pick(["ProgramName", "Programs Full Name"])
    if id_col is None or prg_col is None:
        return pd.DataFrame(columns=["ClientID", "ProgramName", "Month", "CaseNoteCount"])

    df = df.rename(columns={id_col: "ClientID", prg_col: "ProgramName"})

    # Wide month columns formatted like YYYY-MM
    month_cols = [
        c
        for c in df.columns
        if isinstance(c, str)
        and len(c) == 7
        and c[4] == "-"
        and c[:4].isdigit()
        and c[5:].isdigit()
    ]
    if month_cols:
        tmp = df[["ClientID", "ProgramName"] + month_cols].copy()
        long_df = tmp.melt(
            id_vars=["ClientID", "ProgramName"],
            value_vars=month_cols,
            var_name="Month",
            value_name="CaseNoteCount",
        )
        long_df["Month"] = pd.to_datetime(long_df["Month"], format="%Y-%m", errors="coerce")
        long_df["CaseNoteCount"] = pd.to_numeric(long_df["CaseNoteCount"], errors="coerce")
        long_df = long_df.dropna(subset=["ClientID"])
        return long_df[["ClientID", "ProgramName", "Month", "CaseNoteCount"]].copy()

    # Alternate pattern
    count_like = [c for c in df.columns if str(c).startswith("Client Notes - Enrollment Level Count")]
    if count_like:
        today = pd.Timestamp.today().to_period("M")
        periods = [today - i for i in range(len(count_like), 0, -1)]
        long_rows = []
        for i, col in enumerate(count_like):
            period = periods[i] if i < len(periods) else today
            tmp = df[["ClientID", "ProgramName", col]].copy()
            tmp["Month"] = period.to_timestamp()
            tmp = tmp.rename(columns={col: "CaseNoteCount"})
            long_rows.append(tmp)

        out = pd.concat(long_rows, ignore_index=True)
        out["CaseNoteCount"] = pd.to_numeric(out["CaseNoteCount"], errors="coerce")
        out = out.dropna(subset=["ClientID"])
        return out[["ClientID", "ProgramName", "Month", "CaseNoteCount"]].copy()

    return pd.DataFrame(columns=["ClientID", "ProgramName", "Month", "CaseNoteCount"])


# -----------------------------
# Wide reshapes + CES reduction
# -----------------------------
def services_wide_all_months(services: pd.DataFrame) -> pd.DataFrame:
    if services is None or services.empty or "ServicesMonth" not in services.columns:
        return pd.DataFrame()

    svc = services.copy()
    svc["Period"] = pd.to_datetime(svc["ServicesMonth"], errors="coerce").dt.to_period("M")
    svc = svc.dropna(subset=["ClientID", "ProgramName", "Period"])
    svc = svc.groupby(["ClientID", "ProgramName", "Period"], as_index=False)["ServicesCount"].sum()
    svc["Col"] = svc["Period"].apply(lambda p: f"{p.strftime('%B %Y')} Service Notes Count")

    wide = (
        svc.pivot_table(
            index=["ClientID", "ProgramName"],
            columns="Col",
            values="ServicesCount",
            aggfunc="sum",
        )
        .reset_index()
    )
    wide.columns.name = None
    return wide


def case_notes_wide_all_months(notes_long: pd.DataFrame) -> pd.DataFrame:
    if notes_long is None or notes_long.empty or "Month" not in notes_long.columns:
        return pd.DataFrame()

    cn = notes_long.copy()
    cn["Period"] = pd.to_datetime(cn["Month"], errors="coerce").dt.to_period("M")
    cn = cn.dropna(subset=["ClientID", "ProgramName", "Period"])
    cn = cn.groupby(["ClientID", "ProgramName", "Period"], as_index=False)["CaseNoteCount"].sum()
    cn["Col"] = cn["Period"].apply(lambda p: f"{p.strftime('%B %Y')} Case Notes")

    wide = (
        cn.pivot_table(
            index=["ClientID", "ProgramName"],
            columns="Col",
            values="CaseNoteCount",
            aggfunc="sum",
        )
        .reset_index()
    )
    wide.columns.name = None
    return wide


def ces_latest_scored(ces: pd.DataFrame) -> pd.DataFrame:
    cols = ["ClientID", "Latest CES Assessment Date", "CES Assessment Score"]
    if ces is None or ces.empty:
        return pd.DataFrame(columns=cols)

    r = ces.copy()
    r["ClientID"] = r["ClientID"].astype("string")
    r["CES_AssessmentDate"] = pd.to_datetime(r["CES_AssessmentDate"], errors="coerce").dt.normalize()
    r["CES_AssessmentScore"] = pd.to_numeric(r["CES_AssessmentScore"], errors="coerce")
    r = r.dropna(subset=["ClientID", "CES_AssessmentDate", "CES_AssessmentScore"])
    if r.empty:
        return pd.DataFrame(columns=cols)

    r = r.sort_values(["ClientID", "CES_AssessmentDate"], na_position="last")
    latest = (
        r.drop_duplicates(subset=["ClientID"], keep="last")[["ClientID", "CES_AssessmentDate", "CES_AssessmentScore"]]
        .rename(
            columns={
                "CES_AssessmentDate": "Latest CES Assessment Date",
                "CES_AssessmentScore": "CES Assessment Score",
            }
        )
        .reset_index(drop=True)
    )
    latest["CES Assessment Score"] = latest["CES Assessment Score"].round().astype("Int64")
    return latest


# -----------------------------
# File list flags (FIXED)
# -----------------------------
def flags_from_filelist(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    txt = df.get("ClientFileList", pd.Series(index=df.index, dtype="string")).astype("string").fillna("")
    txt_norm = txt.str.replace("’", "'", regex=False)

    def has(needle: str) -> pd.Series:
        return txt_norm.str.contains(needle, case=False, na=False, regex=False)

    income_parts = [
        ("Pay Stub", "Pay Stub"),
        ("SSDI", "Supplemental Security Disability Income (SSDI) Forms"),
        ("SSI", "Supplemental Security Income (SSI) Forms"),
        ("GR", "General Relief (GR) Form"),
        ("Food Stamp", "Food Stamp Card or Award Letter"),
        ("CalWORKS", "CalWORKS Forms"),
        ("Form 1087", "Form 1087 - Self Declaration of Income/No Income Form"),
        ("Form 1084", "Form 1084 - 3rd Party Income Verification"),
        ("Alimony Agreement", "Alimony Agreement"),
        ("Social Security (NUMI) Printout", "Social Security (NUMI) Printout"),
        ("Tax Return", "Tax Return"),
        ("Veterans Affairs (VA) Benefits Award Letter", "Veterans Affairs (VA) Benefits Award Letter"),
        ("Self Employment Document", "Self Employment Document"),
        ("Other Financial Document", "Other Financial Document"),
    ]

    poilist = pd.Series("", index=df.index, dtype="string")
    for label, needle in income_parts:
        mask = has(needle)
        poilist = poilist.mask(mask & (poilist == ""), label)
        poilist = poilist.mask(mask & (poilist != ""), poilist + ", " + label)

    # FIX: Properly prefix when present, "0" when absent
    df["Proof of Income"] = pd.Series("0", index=df.index, dtype="string")
    df.loc[poilist != "", "Proof of Income"] = "1 - " + poilist[poilist != ""]

    hilist = pd.Series("", index=df.index, dtype="string")
    for label, needle in [
        ("Medicare/Medicaid", "Medicaid or Medicare Card"),
        ("Other Health Insurance", "Health Insurance Documentation"),
    ]:
        mask = has(needle)
        hilist = hilist.mask(mask & (hilist == ""), label)
        hilist = hilist.mask(mask & (hilist != ""), hilist + ", " + label)

    df["Health Insurance"] = pd.Series("0", index=df.index, dtype="string")
    df.loc[hilist != "", "Health Insurance"] = "1 - " + hilist[hilist != ""]

    df["SSN Card"] = has("Social Security Card").astype(int)
    df["Birth Certificate"] = has("Birth Certificate or Hospital Record of Birth").astype(int)
    df["CDL or State ID"] = txt_norm.str.contains(
        "Driver's License/State ID Card/Photo ID/ School Identification Card",
        case=False,
        na=False,
        regex=False,
    ).astype(int)
    df["Disability Verification"] = has("Disability Verification").astype(int)

    return df


# -----------------------------
# Final view builder
# -----------------------------
def build_final_view_from_parquets(staging2_dir: Path) -> pd.DataFrame:
    prog_path = staging2_dir / "program_client_data.parquet"
    if not prog_path.exists():
        logging.error(
            "No Program Client Data parquet found. Place the latest file in the Program Client Data folder (or its Archive) and rerun."
        )
        return pd.DataFrame()

    prog = pd.read_parquet(prog_path)
    ces_path = staging2_dir / "ces_assessments.parquet"
    notes_path = staging2_dir / "case_notes.parquet"
    serv_path = staging2_dir / "services.parquet"

    ces = pd.read_parquet(ces_path) if ces_path.exists() else pd.DataFrame()
    notes = pd.read_parquet(notes_path) if notes_path.exists() else pd.DataFrame()
    serv = pd.read_parquet(serv_path) if serv_path.exists() else pd.DataFrame()

    notes_long = notes.copy()
    if not notes_long.empty and "Month" not in notes_long.columns:
        notes_long = preprocess_case_notes_to_long(notes_long)

    notes_wide = case_notes_wide_all_months(notes_long) if not notes_long.empty else pd.DataFrame()
    serv_wide = services_wide_all_months(serv) if not serv.empty else pd.DataFrame()
    ces_latest = ces_latest_scored(ces) if not ces.empty else pd.DataFrame(
        columns=["ClientID", "Latest CES Assessment Date", "CES Assessment Score"]
    )

    df = prog.copy()
    if not notes_wide.empty:
        df = df.merge(notes_wide, how="left", on=["ClientID", "ProgramName"])
    if not serv_wide.empty:
        df = df.merge(serv_wide, how="left", on=["ClientID", "ProgramName"])
    if not ces_latest.empty:
        df = df.merge(ces_latest, how="left", on=["ClientID"])

    df = flags_from_filelist(df)

    today = pd.Timestamp.today().normalize()

    def ces_status(dt):
        if pd.isna(dt):
            return "CES Not Done"
        delta = (today - pd.to_datetime(dt)).days
        return "Renewal Overdue" if delta > 730 else "Current"

    df["CES Status"] = df["Latest CES Assessment Date"].apply(ces_status)

    ces_score = pd.to_numeric(df.get("CES Assessment Score"), errors="coerce")

    df["Document Ready"] = (
        (df.get("CDL or State ID", 0).fillna(0).astype(int) == 1)
        & (df.get("SSN Card", 0).fillna(0).astype(int) == 1)
        & df.get("Proof of Income", "0").astype("string").str.match(r"^1\b", na=False)
    ).map({True: "Document Ready", False: "Not Document Ready"})

    df["Housing Matching Ready"] = (
        (df.get("CDL or State ID", 0).fillna(0).astype(int) == 1)
        & (df.get("SSN Card", 0).fillna(0).astype(int) == 1)
        & df.get("Proof of Income", "0").astype("string").str.match(r"^1\b", na=False)
        & (ces_score.fillna(0) >= 8)
    ).map({True: "Housing Matching Ready", False: "Not Housing Matching Ready"})

    days = pd.to_numeric(df.get("DaysInProject"), errors="coerce")
    docs_ready = df["Document Ready"] != "Not Document Ready"
    low_ces = ces_score.isna() | (ces_score < 8)
    ces_expired = df["CES Status"].eq("Renewal Overdue")
    long_stay = days.ge(120)
    mid_stay = days.ge(75) & days.lt(120)

    alert = pd.Series(pd.NA, index=df.index, dtype="string")
    alert = alert.mask(long_stay & ~docs_ready, "≥120 days & missing docs – ESCALATE")
    alert = alert.mask(long_stay & ces_expired, "≥120 days & CES expired – ESCALATE")
    alert = alert.mask(long_stay & low_ces, "≥120 days & CES<8 – ESCALATE")
    alert = alert.mask(long_stay & ~(~docs_ready | ces_expired | low_ces), "≥120 days & docs/CES OK – monitor")
    alert = alert.mask(mid_stay & ~docs_ready, "75–119 days & missing docs – ACTION")
    alert = alert.mask(mid_stay & (ces_expired | low_ces), "75–119 days & CES risk – ACTION")
    df["Intervention Alert"] = alert

    # Rename back to original export-friendly headers
    rename_back = {
        "ClientID": "Clients Unique Identifier",
        "ActiveInProject": "Enrollments Active in Project",
        "ClientFullName": "Clients Client Full Name",
        "ActiveROI": "Clients Active ROI?",
        "DaysInProject": "Enrollments Days in Project",
        "ProjectStartDate": "Enrollments Project Start Date",
        "POCName": "Client Custom Point of Contact Name",
        "POCPhone": "Client Custom Point of Contact Phone",
        "POCEmail": "Client Custom Point of Contact Email",
        "POCDate": "Client Custom Point of Contact Date",
        "DOBDataQuality": "Clients DoB Data Quality",
        "SSNDataQuality": "Clients SSN Data Quality",
        "TB_ClearanceDate": "Client Assessment Custom TB Clearance Date",
        "ProgramName": "Programs Full Name",
        "LastAssessmentID": "Client Assessments Last Assessment ID",
        "LastAssessmentDate": "Client Assessments Last Assessment Date",
        "AssignedStaff": "List of Assigned Staff",
        "ClientFileList": "List of Client File Name",
    }
    df = df.rename(columns=rename_back)

    # normalize types for final output
    for c in ["SSN Card", "Birth Certificate", "CDL or State ID", "Disability Verification"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0).astype(int)

    if "Proof of Income" in df.columns:
        df["Proof of Income"] = df["Proof of Income"].astype("string").fillna("0")

    if "Bed Number" not in df.columns:
        df["Bed Number"] = pd.NA

    # friendly date formatting
    for col in [
        "Enrollments Project Start Date",
        "Client Custom Point of Contact Date",
        "Client Assessments Last Assessment Date",
        "Latest CES Assessment Date",
        "Client Assessment Custom TB Clearance Date",
    ]:
        if col in df.columns:
            dt = pd.to_datetime(df[col], errors="coerce")
            df[col] = dt.apply(lambda x: f"{x.month}/{x.day}/{x.year}" if not pd.isna(x) else pd.NA)

    front_cols = [
        "Clients Unique Identifier",
        "Enrollments Active in Project",
        "Clients Client Full Name",
        "Clients Active ROI?",
        "Enrollments Days in Project",
        "Enrollments Project Start Date",
        "Client Custom Point of Contact Name",
        "Client Custom Point of Contact Phone",
        "Client Custom Point of Contact Email",
        "Client Custom Point of Contact Date",
        "Clients DoB Data Quality",
        "Clients SSN Data Quality",
        "Client Assessment Custom TB Clearance Date",
        "Programs Full Name",
        "Client Assessments Last Assessment ID",
        "Client Assessments Last Assessment Date",
        "List of Assigned Staff",
        "SSN Card",
        "Birth Certificate",
        "CDL or State ID",
        "Disability Verification",
        "Proof of Income",
        "Bed Number",
    ]
    month_cols = [c for c in df.columns if c.endswith(" Case Notes") or c.endswith(" Service Notes Count")]
    tail_cols = [
        "Latest CES Assessment Date",
        "CES Assessment Score",
        "CES Status",
        "Document Ready",
        "Housing Matching Ready",
        "Intervention Alert",
        "SourceFile",
        "SourceDataset",
    ]

    for c in front_cols + month_cols + tail_cols:
        if c not in df.columns:
            df[c] = pd.NA

    return df[front_cols + month_cols + tail_cols].copy()


# -----------------------------
# Atomic writing
# -----------------------------
def atomic_replace(src: Path, dst: Path, attempts: int = 12, delay: int = 2) -> None:
    last_err = None
    for _ in range(attempts):
        try:
            src.replace(dst)
            return
        except PermissionError as e:
            last_err = e
            time.sleep(delay)
    if last_err:
        raise last_err


def atomic_write_parquet(df: pd.DataFrame, path: Path, **kwargs) -> None:
    path = Path(path)
    ensure_dir(path.parent)
    with NamedTemporaryFile("wb", delete=False, dir=path.parent, suffix=".tmp") as tf:
        tmp = Path(tf.name)
    df.to_parquet(tmp, **kwargs)
    atomic_replace(tmp, path)


def atomic_write_csv(df: pd.DataFrame, path: Path, **kwargs) -> None:
    path = Path(path)
    ensure_dir(path.parent)
    encoding = kwargs.pop("encoding", "utf-8-sig")
    newline = kwargs.pop("newline", "")
    with NamedTemporaryFile("w", delete=False, dir=path.parent, suffix=".tmp", encoding=encoding, newline=newline) as tf:
        tmp = Path(tf.name)
    df.to_csv(tmp, **kwargs)
    atomic_replace(tmp, path)


def write_excel_splitting(
    df: pd.DataFrame,
    out_path_base: Path,
    sheet_name: str = "Sheet1",
    max_rows: int = 1_000_000,
) -> None:
    ensure_dir(out_path_base.parent)
    n = len(df)

    if n == 0:
        dst = out_path_base.with_suffix(".xlsx")
        with NamedTemporaryFile("wb", delete=False, dir=dst.parent, suffix=".tmp.xlsx") as tf:
            tmp = Path(tf.name)
        with pd.ExcelWriter(tmp, engine="openpyxl") as xw:
            pd.DataFrame().to_excel(xw, index=False, sheet_name=sheet_name)
        atomic_replace(tmp, dst)
        return

    parts = (n + max_rows - 1) // max_rows
    if parts == 1:
        dst = out_path_base.with_suffix(".xlsx")
        with NamedTemporaryFile("wb", delete=False, dir=dst.parent, suffix=".tmp.xlsx") as tf:
            tmp = Path(tf.name)
        with pd.ExcelWriter(tmp, engine="openpyxl") as xw:
            df.to_excel(xw, index=False, sheet_name=sheet_name)
        atomic_replace(tmp, dst)
        return

    for i in range(parts):
        lo = i * max_rows
        hi = min((i + 1) * max_rows, n)
        part_df = df.iloc[lo:hi].copy()
        dst = out_path_base.with_name(out_path_base.stem + f"_part{i+1}").with_suffix(".xlsx")
        with NamedTemporaryFile("wb", delete=False, dir=dst.parent, suffix=".tmp.xlsx") as tf:
            tmp = Path(tf.name)
        with pd.ExcelWriter(tmp, engine="openpyxl") as xw:
            part_df.to_excel(xw, index=False, sheet_name=sheet_name)
        atomic_replace(tmp, dst)


# -----------------------------
# Dataset processing (latest only + caching)
# -----------------------------
def process_dataset(
    ds_cfg: Dict[str, Any],
    staging2_dir: Path,
    state_dir: Path,
    reader_defaults: Dict[str, Any],
    dry_run: bool = False,
    force: bool = False,
) -> pd.DataFrame:
    name = ds_cfg["name"]
    staging1 = Path(ds_cfg["staging1"])
    include = ds_cfg.get("include_patterns", ["*.csv", "*.xlsx"])
    exclude = ds_cfg.get("exclude_patterns", [])
    ensure_dir(staging1)
    ensure_dir(staging2_dir)
    ensure_dir(state_dir)

    archive_dir = staging1 / "Archive"

    files = list_ingest_files(staging1, include, exclude)
    if archive_dir.exists():
        files += list_ingest_files(archive_dir, include, exclude)

    files = sorted(set(files))
    latest = pick_latest_file(files)
    logging.info(f"[{name}] Found {len(files)} file(s); using latest: {latest.name if latest else 'None'}")

    if latest is None:
        return pd.DataFrame()

    out_path = staging2_dir / ds_cfg.get("combined_filename", f"{name}.parquet")
    state_path = state_dir / f"{name}.json"

    # Cache check
    try:
        latest_md5 = md5_file(latest)
    except Exception as e:
        logging.warning(f"[{name}] Could not compute MD5 for cache check: {e}")
        latest_md5 = None

    if not force and latest_md5 and out_path.exists():
        prev = load_state(state_path)
        if prev.get("md5") == latest_md5 and prev.get("source_file") == latest.name:
            logging.info(f"[{name}] No change detected (MD5 match). Skipping ingest; using existing parquet.")
            return pd.read_parquet(out_path)

    # Read
    try:
        if latest.suffix.lower() == ".csv":
            df = read_csv(latest, reader_defaults)
        elif latest.suffix.lower() in [".xlsx", ".xls"]:
            df = read_excel(latest, reader_defaults)
        else:
            logging.info(f"[{name}] Unsupported file type: {latest.name}")
            return pd.DataFrame()
    except ImportError as e:
        logging.error(f"[{name}] Excel read failed due to missing dependency: {e}")
        logging.error(f"[{name}] If your files are .xls, you may need to install 'xlrd'.")
        return pd.DataFrame()
    except Exception as e:
        logging.exception(f"[{name}] Failed to read {latest.name}: {e}")
        return pd.DataFrame()

    if df is None or df.empty:
        logging.info(f"[{name}] Empty/unreadable: {latest.name}")
        return pd.DataFrame()

    # Transform
    df = normalize_columns(df)
    df = ensure_unique_columns(df)

    if name == "case_notes":
        try:
            df = preprocess_case_notes_to_long(df)
        except Exception as e:
            logging.exception(f"[{name}] Preprocess failed for {latest.name}: {e}")
            return pd.DataFrame()

    df["SourceFile"] = latest.name
    df["SourceDataset"] = name

    df = apply_column_renames(df, ds_cfg.get("renames", {}))
    df = parse_dates(df, ds_cfg.get("date_rules", []))
    df = enforce_dtypes(df, ds_cfg.get("dtypes", {}))
    df = trim_strings(df)
    df = to_lower(df, ds_cfg.get("lowercase_columns", []))
    df = drop_bad_rows(df, ds_cfg.get("drop_queries", []), ds_cfg.get("drop_null_any", []))
    df = deduplicate(df, ds_cfg.get("dedup_keys", []), ds_cfg.get("recency_cols", []))

    if dry_run:
        logging.info(f"[{name}] [Dry-run] Would write latest parquet to {out_path} ({len(df):,} rows).")
        return df

    df.to_parquet(out_path, index=False)
    logging.info(f"[{name}] Wrote latest parquet to {out_path} ({len(df):,} rows).")

    # Save state
    if latest_md5:
        save_state(
            state_path,
            {
                "source_file": latest.name,
                "md5": latest_md5,
                "rows": int(len(df)),
                "timestamp": pd.Timestamp.now().isoformat(),
            },
        )

    return df


# -----------------------------
# Pipeline runner
# -----------------------------
def run_pipeline(cfg: Dict[str, Any], dry_run: bool = False, force: bool = False) -> None:
    logging.basicConfig(level=getattr(logging, cfg.get("logging", {}).get("level", "INFO")))
    logging.info("=== Starting HMIS ETL (latest files only; no historical) ===")

    staging2_dir = Path(cfg["paths"]["staging2"])
    output_dir = Path(cfg["paths"]["output"])
    state_dir = Path(cfg["paths"]["state_dir"])

    for p in [staging2_dir, output_dir, state_dir]:
        ensure_dir(p)

    reader_defaults = cfg["reader_defaults"]
    datasets = cfg["datasets"]

    for ds in datasets:
        process_dataset(ds, staging2_dir, state_dir, reader_defaults, dry_run=dry_run, force=force)

    if dry_run:
        logging.info("[Dry-run] Skipping outputs.")
        logging.info("=== Done (dry-run) ===")
        return

    curated = build_final_view_from_parquets(staging2_dir)
    if curated.empty:
        logging.error(
            "Final view is empty because required latest files were not found. "
            "Add the latest HMIS files (CSV/XLSX) to the staging folders (or their Archive subfolders) and rerun."
        )
        return

    vb = cfg["output"].get("final_view_basename", "WCA_Biweekly_Final")

    if cfg["output"].get("write_parquet", True):
        atomic_write_parquet(curated, output_dir / f"{vb}.parquet", index=cfg["output"].get("index", False))

    if cfg["output"].get("write_csv", True):
        atomic_write_csv(curated, output_dir / f"{vb}.csv", index=cfg["output"].get("index", False))

    if cfg["output"].get("write_excel", True):
        write_excel_splitting(curated, output_dir / vb)

    logging.info(f"[output] Wrote {vb}.* to {output_dir}")
    logging.info("=== Pipeline complete ===")


def main() -> None:
    ap = argparse.ArgumentParser(description="Biweekly HMIS ETL (latest files only)")
    ap.add_argument("--dry-run", action="store_true", help="Run ingest/transform without writing outputs.")
    ap.add_argument("--force", action="store_true", help="Ignore cache and re-ingest latest files.")
    ap.add_argument(
        "--base",
        default=None,
        help="Base folder for the QA Data Hub. Overrides HMIS_BASE environment variable.",
    )
    ap.add_argument("--log-level", default=None, help="Override logging level (e.g., INFO, DEBUG).")
    args = ap.parse_args()

    base = args.base or Path(str(Path.cwd())).as_posix()
    # allow env override without importing os (avoid accidental path leakage in logs)
    try:
        import os as _os  # local import only

        base = args.base or _os.environ.get("HMIS_BASE") or DEFAULT_BASE
    except Exception:
        base = args.base or DEFAULT_BASE

    cfg = make_cfg(base)

    if args.log_level:
        cfg["logging"]["level"] = args.log_level.upper()

    run_pipeline(cfg, dry_run=args.dry_run, force=args.force)


if __name__ == "__main__":
    main()
